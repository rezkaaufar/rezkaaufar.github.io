<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2026-02-22T23:00:40+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Rezka Leonandya</title><subtitle>Rezka Leonandya&apos;s personal site and blog.</subtitle><author><name>Rezka Leonandya</name></author><entry><title type="html">Non-compliance issue in an AB test</title><link href="http://localhost:4000/compliance-issue-in-ab-test/" rel="alternate" type="text/html" title="Non-compliance issue in an AB test" /><published>2026-02-12T00:00:00+07:00</published><updated>2026-02-12T00:00:00+07:00</updated><id>http://localhost:4000/compliance-issue-in-ab-test</id><content type="html" xml:base="http://localhost:4000/compliance-issue-in-ab-test/"><![CDATA[<p>In a recent AB test that I did, I did the usual calculation of sample size, power, alpha, and MDE. At the end of the test, the test result was statistically significant. However, I then realized that some units in the treatment group were not actually doing the treatment. They were given the treatment but not all of them actually did it. This is a compliance issue that we need to address.</p>

<h1 id="non-compliance-issue-makes-your-test-underpowered">Non-compliance issue makes your test underpowered</h1>

<p>We first looked at the average treatment effect between the treatment group and control group:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">smf</span><span class="p">.</span><span class="nf">ols</span><span class="p">(</span><span class="sh">'</span><span class="s">conversion ~ treatment</span><span class="sh">'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">).</span><span class="nf">fit</span><span class="p">().</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<p>The test result turn out to be statistically significant. But, given not all of the units in the treatment group actually did the treatment, our test is actually underpowered. Now what does this mean?</p>

<p>During the sample size calculation, when we set an initial Minimum Detectable Effect (MDE), we are defining the smallest “signal” we want to be able to see through the “noise” (variance) of your data. However, this rests under the assumption that all the treatment unit are actually the treated unit. In a non-compliance case, only a fraction of our treatment unit are actually the treated unit. Because of this non-compliers, the effect we actually observe in our dashboard is smaller than the true effect on the unit that actually did the treatment. The observed effect goes by the name of Intention-to-Treat (ITT) effect, while the true effect on the unit that actually did the treatment goes by the name of Local Average Treatment Effect (LATE).</p>

<p>Hence for our MDE to be valid, we need to adjust it based on the compliance rate. Assume for this post, our compliance rate is 60%.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mde_target</span> <span class="o">=</span> <span class="mf">0.02</span>      <span class="c1"># The MDE that we set, targeted for compliers
</span><span class="n">compliance</span> <span class="o">=</span> <span class="mf">0.6</span>       <span class="c1"># 60% compliance rate
</span><span class="n">true_itt</span> <span class="o">=</span> <span class="n">mde_target</span> <span class="o">*</span> <span class="n">compliance</span>  <span class="c1"># actual effect size: 1.2% ITT lift
</span></code></pre></div></div>

<p>So in our case, when we set our MDE to 2%, this is an MDE for the compliers. For the whole treatment group (or the ITT), we are actually targeting for 1.2% ITT lift. Hence the sample size we calculated is supposed to be done using the 1.2% ITT lift, not the 2% MDE.</p>

<p>Below is the thing we need to calculate for our sample size calculation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># Standard deviation of your metric
</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">target_power</span> <span class="o">=</span> <span class="mf">0.8</span>

<span class="c1"># Z-scores
</span><span class="n">z_alpha</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">z_beta</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">ppf</span><span class="p">(</span><span class="n">target_power</span><span class="p">)</span>

<span class="c1"># Sample Size Calculations
</span><span class="n">n_wrong</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">z_alpha</span> <span class="o">+</span> <span class="n">z_beta</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">mde</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">n_correct</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">n_wrong</span> <span class="o">/</span> <span class="p">(</span><span class="n">compliance</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">se_diff_wrong</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n_wrong</span><span class="p">)</span>
<span class="n">se_diff_correct</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n_correct</span><span class="p">)</span>

<span class="n">threshold_wrong</span> <span class="o">=</span> <span class="n">z_alpha</span> <span class="o">*</span> <span class="n">se_diff_wrong</span>
<span class="n">threshold_correct</span> <span class="o">=</span> <span class="n">z_alpha</span> <span class="o">*</span> <span class="n">se_diff_correct</span>
</code></pre></div></div>

<p>And here is how our distribution of mean difference looked like when we use the wrong sample size and the correct sample size:</p>

<p><img src="/assets/img/non-compliance-ab-test.png" alt="Min sample size" />
<em>Figure 1: Comparison of mean difference distribution when using wrong and correct sample size</em></p>

<p>Notice that here we are not using the MDE as our H1 mean. Instead, we are using the true ITT lift as our H1 mean. This is because we are interested in the effect on the whole treatment group, not just the compliers.</p>

<p>From the image above, we can see that if we use the wrong sample size, the distribution is much wider. The orange area is massive, and the significance threshold (threshold_wrong) is to the right of the H1 mean. This means that even if we observe the True ITT during our test, we would fail to reject the null hypothesis. In this situation, we are more likely to make a Type II error (false negative).</p>

<p>Once we use the correct sample size by applying the $1/c^2$ correction (common correction on non-compliance issue), the distribution becomes narrower. The significance threshold shifts to the left (threshold_correct)  and we can see that we now have 80% power to detect the treatment effect.</p>

<p>Now back to my original AB test, our test was underpowered but we still got a statistically significant result. Does this mean that we can conclude that the treatment is effective? Not at all. We just got lucky. If our “true” effect is actually 0.012, our data had to “get lucky” and swing into that thin green tail on the right side of the signicant threshold to be significant. This is the definition of Magnitude Error: our result is significant only because it is an outlier that is larger than the true average.</p>

<p>We can also do some simulation to check the average of the significant results from both the wrong and correct sample size test.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulation
</span><span class="n">n_sims</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="n">results_wrong</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">true_itt</span><span class="p">,</span> <span class="n">se_diff_wrong</span><span class="p">,</span> <span class="n">n_sims</span><span class="p">)</span>
<span class="n">results_correct</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">true_itt</span><span class="p">,</span> <span class="n">se_diff_correct</span><span class="p">,</span> <span class="n">n_sims</span><span class="p">)</span>

<span class="c1"># Determine significance (two-tailed)
</span><span class="n">sig_wrong</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">results_wrong</span> <span class="o">/</span> <span class="n">se_diff_wrong</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">z_alpha</span>
<span class="n">sig_correct</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">results_correct</span> <span class="o">/</span> <span class="n">se_diff_correct</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">z_alpha</span>
</code></pre></div></div>

<p><img src="/assets/img/non-compliance-ab-test-simulation.png" alt="Simulation" />
<em>Figure 2: Simulation of the observed effect when using wrong and correct sample size</em></p>

<p>We can see that in our underpowered test, our average significant result is 1.90, closer to our original MDE of 2%. Whereas in the correct sample size test, our average significant result is 1.20, closer to our true ITT lift of 1.2%. This is inline with our previous analysis where the significance threshold of the underpowered test is higher than the significane threshold of the correct sample size test.</p>

<h1 id="what-if-we-dont-know-the-compliance-rate">What if we don’t know the compliance rate?</h1>

<p>In an ideal world, we know the compliance rate before doing an AB Test. But there could be a situation where we don’t know the compliance rate before doing an AB test. In this case, I envision these two options:</p>

<ul>
  <li>Pilot phase: where we do small rollout in the beginning to 1% of the population to estimate the compliance rate. Then we use the estimated compliance rate to calculate the required sample size for the full test. Note that here ideally we need to reset the experiment after the pilot phase. If we keep the 1% running and just add the remaining 99%, we are technically performing sequential testing. Standard t-tests does not work as doing this inflates the Type I error rate.</li>
  <li>Historical proxy: If we are testing a new “Checkout” button, look at the last 30 days of data. What percentage of users who landed on the cart page actually clicked the current checkout button? Use that as the compliance rate.</li>
</ul>

<p>I think there might be other options but these are the two that I am familiar with.</p>

<h1 id="relation-to-instrumental-variable">Relation to instrumental variable</h1>

<p>The $1/c^2$ correction actually comes from the instrumental variable (IV) framework. In the IV framework, we are interested in the effect of the treatment (T) on the outcome (Y). However, we only have access to the treatment (T) and the instrument (Z). So what we do is to estimate the effect of the instrument (Z) on the treatment (T) and use that to estimate the effect of the treatment (T) on the outcome (Y).</p>

<p>In an AB test with non-compliance issue, we can use the randomization (Z) as the instrument. We can do this because in this case, randomization $\neq$ participation. Following the IV framework math, we will get the compliance rate that can be interpreted as the correlation between the instrument (randomization) and the treatment (T). This is usually known as the first stage in an IV framework. The second stage (reduced form) is when we divide the first stage by the compliance rate to get the ITT effect.</p>]]></content><author><name>Rezka Leonandya</name></author><category term="statistics" /><category term="experimentation" /><summary type="html"><![CDATA[In a recent AB test that I did, I did the usual calculation of sample size, power, alpha, and MDE. At the end of the test, the test result was statistically significant. However, I then realized that some units in the treatment group were not actually doing the treatment. They were given the treatment but not all of them actually did it. This is a compliance issue that we need to address.]]></summary></entry><entry><title type="html">Notes on doing AB test properly</title><link href="http://localhost:4000/ab-testing-notes/" rel="alternate" type="text/html" title="Notes on doing AB test properly" /><published>2026-02-07T00:00:00+07:00</published><updated>2026-02-07T00:00:00+07:00</updated><id>http://localhost:4000/ab-testing-notes</id><content type="html" xml:base="http://localhost:4000/ab-testing-notes/"><![CDATA[<p>I have known AB Testing and used it for a while in my work but sometimes I find myself remembering the equation too much and unsure about how things worked under the hood. In this post I try to go over the concept and visualize it to make it easier to remember.</p>

<h1 id="mde-power-alpha-and-sample-size">MDE, power, alpha, and sample size</h1>

<p>We start off by looking at the relationship between minimum detectable effect (MDE), power (1-beta), alpha, and sample size. In an AB test, we set MDE, power, and alpha to calculate the minimum sample size required to run the AB test.</p>

<p>Here is an image showing the relationship between the 4 variables:</p>

<p><img src="/assets/img/ab-test-triangle.png" alt="Statistical Power Analysis Triangle" /></p>

<p><em>Figure 1: Statistical power analysis triangle. A change in one variable requires fixing the other two</em></p>

<p>Before we go to the sample size calculation, we start off by varying sample size and see how it affects the distributions of the control and treatment groups.</p>

<p>We begin with two distributions of a conversion metrics, one for the control group and one for the treatment group. Here is the detail of our distributions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_base</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">p_treat</span> <span class="o">=</span> <span class="mf">0.12</span>
<span class="n">se0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p_base</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_base</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
<span class="n">se1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p_treat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_treat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
</code></pre></div></div>

<p>Here we assume that our control group conversion is 10% and our target treatment group conversion is 12%. We can plot both our distribution above with varying sample size n:</p>

<p><img src="/assets/img/control-v-treatment.png" alt="Control vs treatment" /></p>

<p><em>Figure 2: Control versus treatment distribution</em></p>

<p>We can see that as the sample size increases, the gap between the two distributions also increases. This is because the standard error decreases as the sample size increases. And this will affect our AB testing later.</p>

<p>Now that we have the control and treatment distribution, we can do a hypothesis testing by constructing a third distribution, which is the distribution of difference in the means. I will not go over the theory too deep here, but this approach is based on the Central Limit Theorem, where the distribution of the difference between two means will approach a normal distribution as the sample size increases.</p>

<p>For the hypothesis testing, we calculate three components: the standard error of the difference (se_diff), the null hypothesis (mu0), and the alternative hypothesis (mu1).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Standard Errors
</span><span class="n">se0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p_base</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_base</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
<span class="n">se1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p_treat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_treat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
<span class="n">se_diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">se0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">se1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">mu0</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mu1</span> <span class="o">=</span> <span class="n">mde</span>
</code></pre></div></div>

<p>We set mu0 to 0 because this is our null hypothesis. We assume that there is no mean difference in both control and treatment, whereas mu1 is our MDE, which is the effect that we want to see. Technically, mu1 is calculated like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean_diff</span> <span class="o">=</span> <span class="n">p_treat</span> <span class="o">-</span> <span class="n">p_base</span>
<span class="n">mu1</span> <span class="o">=</span> <span class="n">mean_diff</span> <span class="o">-</span> <span class="n">mu0</span>
</code></pre></div></div>

<p>which basically is equal to our MDE. In this phase where we haven’t done any AB testing, we assume that the treatment group is better than the control group by MDE.</p>

<p>Now we can plot the distribution of this mean difference by varying the sample size and observe how it affects our power and how does the shape of the distribution change as we adjust the sample size.</p>

<p><img src="/assets/img/ab-testing-distributions.png" alt="AB testing distributions" />
<em>Figure 3: Comparison of mean difference distributions with varying samples</em></p>

<p>The solid black line (threshold) represents the critical value, which is the value that we use to determine if our result is statistically significant. This black line is calculated based on the z-score of the alpha value and the standard error of the difference in means.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Significance Threshold (Critical Value)
</span><span class="n">z_alpha</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="nf">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>
<span class="n">x_crit</span> <span class="o">=</span> <span class="n">mu0</span> <span class="o">+</span> <span class="n">z_alpha</span> <span class="o">*</span> <span class="n">se_diff</span>
</code></pre></div></div>

<p>We can see at $n=200$ the threshold is actually to the right of the green dashed line (the MDE). This shows that with such a small sample, even if we hit our exact target of $0.02$, it wouldn’t be “statistically significant.” We would need a massive, lucky fluke to actually reject the null. As $n$ hits $20,000$, the black threshold line moves significantly to the left of our $0.02$ target, meaning almost any result near our MDE will be correctly identified as a winner.</p>

<p>Notice how the power changes as we have increased sample as well. Power is the entire rest of the green curve to the right of the significant line. It represents all the times the treatment actually worked and we correctly identified it as a winner. If we have power of 80%, this means that if the 2% lift is real, we will correctly identify it 4 out of 5 times.</p>

<p>Now we can see that there is indeed a relationship between the MDE, alpha, beta and the minimum sample size required. For a test to be valid in detecting our MDE, we need to have a sample size that is large enough to reject the null hypothesis.</p>

<h1 id="deciding-sample-sizes">Deciding sample sizes</h1>

<p>As mentioned above, in an AB test what we want is to determine the minimum sample size to be able to conclude an experiment given the experiment config. Hence, what we usually do is that we set alpha, beta (power), and our MDE to calculate how many samples do we need to run the AB test.</p>

<p>Here is how the image looks if we do one-tail test with alpha=5%, beta=20%, and MDE of 0.02</p>

<p><img src="/assets/img/one-tail-min-sample-size.png" alt="Min sample size" />
<em>Figure 4: Distribution of meandifference when by setting alpha=5%, beta=20%, and MDE of 0.02</em></p>

<p>Here we observe the following:</p>

<p>At $n=3,024$, the significant threshold is positioned perfectly so that 5% of the Blue Curve is to its right (our $\alpha$ or False Positive risk) and 20% of the Green Curve is to its left (our $\beta$ or False Negative risk)</p>

<p>We can also observe that he curves are just “skinny” enough that the Green Shaded Area (Power) covers exactly 80% of the Treatment distribution. This means if the 2% lift is  real, we will correctly identify it 4 out of 5 times.</p>

<h1 id="concluding-the-experiment">Concluding the experiment</h1>

<p>Once we have our AB test results, we can conclude the experiment by checking if the result is statistically significant.</p>

<p>With the observed $\hat{p}_1$ from our treatment group $\hat{p}_0$ from our control group, and $SE_{observed}$ as the standard error of the difference in means, we calculate z-score</p>

\[Z = \frac{(\hat{p}_1 - \hat{p}_0) - 0}{SE_{observed}}\]

<p>If $Z$ &gt; $Z_{\alpha}$ (1.96 in $N(0,1)$), then we can reject the null hypothesis and conclude that the treatment is statistically significant.</p>

<p>Another way to look at it is to look at our observed $\hat{p}_1$ and if it is above the black threshold ($x_{crit} = 0 + 1.96 \cdot SE_{diff}$) ($\approx 0.013$), then we can reject the null hypothesis and conclude that the treatment is statistically significant.</p>

<h1 id="what-to-do-if-your-test-is-statsig-but-effect-size-is-below-your-mde">What to do if your test is statsig but effect size is below your MDE?</h1>

<p>In this case, I believe we can reject the null hypothesis. If our result lands slightly to the left of the “True Effect” ($0.02$), as long as it stays to the right of the black threshold ($\approx 0.013$), we can still detect it as statistical significant. It is still good to proceed with the treatment if we observe an increase in our metrics.</p>

<p>If the effect size is positive but it is below the significance threshold (not statistically significant), then this warrants a further decision and investigation. It could be that the true effect size is indeed smaller than the MDE and requires more sample size to detect it. We can consider to continue the experiment with higher sample size if the cost of running the experiment is low and the potential benefit is high. Otherwise, we can consider to stop the experiment and try again with a different approach.</p>

<p>The other case where we should not proceed with the treatment is when we do not reach statistical significance or if the effect size is negative. Negative effect means that the outcome of the treatment is worse than the control.</p>

<h1 id="what-happens-if-we-have-unequal-split-between-control-and-treatment">What happens if we have unequal split between control and treatment?</h1>

<p>Lets say we have 90% control and 10% treatment.</p>

<p><img src="/assets/img/imbalanced-ab-test.png" alt="Imbalanced split" />
<em>Figure 5: Distribution of difference when by setting alpha=5%, beta=20%, and MDE of 0.02 and unequal split between control and treatment</em></p>

<p>In this case, the standard error of the distribution of the mean difference increases, which means that we need a larger sample size to achieve the same level of statistical significance. So an unequal split in an AB test requires higher samples.</p>

\[SE_{diff} = \sqrt{\frac{\sigma^2_{control}}{\mathbf{n_{small}}} + \frac{\sigma^2_{treatment}}{n_{large}}}\]

<p>From the formula above, a tiny sample in either group will blow up the total error.</p>

<p>Same case can also be said if we have 10% control and 90% treatment. Even though here we are aggresively rolling out to new users and impacting the main metrics already, the changes cannot be fully trusted until we have enough sample to reach statistical significance.</p>]]></content><author><name>Rezka Leonandya</name></author><category term="statistics" /><category term="experimentation" /><summary type="html"><![CDATA[AB testing is a common method used in tech companies to evaluate the effectiveness of different strategies. In this post I try to go over the concept and visualize it to make it easier to remember.]]></summary></entry><entry><title type="html">How to interpret confidence intervals (correctly)</title><link href="http://localhost:4000/confidence-intervals/" rel="alternate" type="text/html" title="How to interpret confidence intervals (correctly)" /><published>2026-01-18T00:00:00+07:00</published><updated>2026-01-18T00:00:00+07:00</updated><id>http://localhost:4000/confidence-intervals</id><content type="html" xml:base="http://localhost:4000/confidence-intervals/"><![CDATA[<h1 id="confidence-intervals">Confidence intervals</h1>

<p>Confidence interval is a range of values that is likely to contain an unknown population parameter. If we were to repeat an experiment or a sampling process many times and calculate a confidence interval each time, 95% of those generated intervals would contain the true population parameter.</p>

<p>Here we visualize our confidence interval when we sample multiple times from a true population. The true population is a normal distribution with mean 100 and standard deviation of 15.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  
<span class="n">pop_mean</span> <span class="o">=</span> <span class="mi">100</span>      <span class="c1"># True population mean
</span><span class="n">pop_std</span> <span class="o">=</span> <span class="mi">15</span>       <span class="c1"># True population standard deviation
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">50</span>      <span class="c1"># Number of times we draw a sample
</span><span class="n">confidence</span> <span class="o">=</span> <span class="mf">0.95</span>   <span class="c1"># 95% Confidence Level
</span></code></pre></div></div>

<p>Here is the result of our confidence interval visualization using 30 sample size:</p>

<p><img src="/assets/img/small_sample_ci.png" alt="Small Sample CI" /></p>

<p><em>Figure 1: Confidence Interval with small sample size</em></p>

<p>We can see that of all the 50 sampling process, we have 3 confidence interval that does not contain the true population mean. This is in line with the 95% confidence level, where we expect 5% of the intervals to not contain the true population parameter.</p>

<p>Here we look at the result of our confidence interval visualization using 100 sample size:</p>

<p><img src="/assets/img/large_sample_ci.png" alt="Large Sample CI" /></p>

<p><em>Figure 2: Confidence Interval with large sample size</em></p>

<p>With larger sample size, we expect to have more confidence intervals that contain the true population parameter. This is because larger sample size provides more information about the population, leading to more accurate estimates of the population parameter.</p>

<p>In the real world, we almost never have the luxury of repeating an experiment multiple times to see the distribution. We get one shot, one dataset, and one set of results. We can then turn to the Central Limit Theorem (CLT) to treat our single experiment as if it were part of that multiple sampling process. The Central Limit Theorem (CLT) states that the distribution of sample means will approach a normal distribution as the sample size increases, regardless of the shape of the population distribution. With the assumption that our single experiment is “typical” and not one of the rare outliers, we can use the CLT to construct a confidence interval for our population parameter and further do hypothesis testing from it.</p>

<h1 id="confidence-intervals-in-linear-regression">Confidence intervals in linear regression</h1>

<p>When doing linear regression, we also got confidence intervals and standard deviation for our estimated parameters. The standard deviation is calculated as such:</p>

\[\hat{\sigma}^2 = \frac{\sum (y_i - \hat{y}_i)^2}{n - k}\]

\[\text{Var}(\hat{\beta}) = \hat{\sigma}^2 (X^T X)^{-1}\]

\[SE(\hat{\beta}_j) = \sqrt{\text{Var}(\hat{\beta}_j)}\]

<p>where $\hat{\sigma}^2$ is the residual variance of the error term, $X$ is the independent variables, and $k$ is the number of parameters in the model.</p>

<p>Now, how can we interpret confidence intervals and standard deviation in linear regression?</p>

<p>For confidence intervals, this essentially means that if we were to fit a line many times, we can expect that 95% of the confidence interval contain the true parameter. For standard deviation, the theory behind it is also the same: if we took many different samples and fit a line to each, how much would the slope of that line vary?</p>

<p>Imagine the “true” relationship between two variables is a fixed, invisible line: $y = 5x + 2$. In the real world, we never see this line; we only see noisy data points scattered around it.</p>

<ul>
  <li>Sample 1: We collect 100 data points and fit a line. The slope estimate is $5.2$, and the 95% CI is [4.8, 5.6]. (This interval contains the true value of 5). The standard deviation of the slope is $0.4$.</li>
  <li>Sample 2: We collect another 100 points. The slope is $4.9$, and the CI is [4.5, 5.3]. (This interval also contains 5). The standard deviation of the slope is $0.3$.</li>
  <li>Sample 100: We collect 100 more points. This time, by pure random chance, we got a “weird” sample. The slope is $6.1$ and the CI is [5.7, 6.5]. (This interval misses the true value of 5). The standard deviation of the slope is $0.5$.</li>
</ul>

<p>If we did this forever, 95% of those calculated brackets would successfully “capture” that true value of 5.</p>

<p>Here is the visualization of the confidence interval for the slope of the line:</p>

<p><img src="/assets/img/linear_regression_ci.png" alt="CI" /></p>

<p><em>Figure 3: Confidence Interval for the slope of the line</em></p>

<p>Another thing to point is that in linear regression, we can view the parameter as the mean change in $Y$ for a unit change in $X$. So the mean of the distribution is here is bidimensional whereas the mean of the distribution in the previous example is unidimensional.</p>]]></content><author><name>Rezka Leonandya</name></author><category term="statistics" /><summary type="html"><![CDATA[Notes on how to interpret confidence intervals.]]></summary></entry><entry><title type="html">Lexical match in a search engine retrieval</title><link href="http://localhost:4000/retrieval-tf-idf/" rel="alternate" type="text/html" title="Lexical match in a search engine retrieval" /><published>2025-04-29T00:00:00+07:00</published><updated>2025-04-29T00:00:00+07:00</updated><id>http://localhost:4000/retrieval-tf-idf</id><content type="html" xml:base="http://localhost:4000/retrieval-tf-idf/"><![CDATA[<p>Have you ever wondered how search engine works? How can it match a query with millions of documents super fast and return them to the users?</p>

<p>In this post, we are going to look at one of the most fundamental technique in search retrieval: lexical match.</p>

<p><img src="/assets/resized/search-query-document-1400x459.png" alt="Figure 1: Query and document." /></p>

<p><em>Figure 1: Query and document..</em></p>

<hr />

<h1 id="retrieval">Retrieval</h1>
<p>In retrieval, we want to return a smaller pool of documents from the huge pools given a query. Imagine that in a system where we have millions of documents and we want to retrieve a subset of documents that are relevant to a given query. The subset essentially is a smaller pool of documents that we want to do further processing on. The goal of the system is to reduce the number of document candidates that can potentially be of interest according to a given query.</p>

<p>There are many techniques to do retrieval, and one of the oldest and battle-tested technique is to use lexical match. The key point is retrieval needs to be fast. In Big-O notation, we want the retrieval to have a time complexity in $\mathcal{O}(1)$ or at most in $\mathcal{O}(\log N)$. Higher time complexity yields to slow and unpleasant experience for users.</p>

<h2 id="retrieval-with-inverted-index">Retrieval with inverted index</h2>

<p>For the sake of simplicity, let’s assume that the query and documents that we have are of string types. To do retrieval fast, we create an inverted index which stores every unique token (vocabulary) that can be found in the document pool and points to document subset containing this token.</p>

<p><img src="/assets/resized/search-inverted-index-1400x631.png" alt="Figure 2: Inverted index." /></p>

<p><em>Figure 2: Inverted index.</em></p>

<p>So instead of doing a string match for every query term to the document term, we can directly access the index to return the list of documents of matched query in $\mathcal{O}(1)$ time complexity.</p>

<p>In popular search libraries such as Elasticsearch, we can make further adjustments on the query and documents preprocessing and on how to define a match between a query and a document. These two techniques is going to affect how the inverted index is built and how the documents are retrieved.</p>

<p>We can preprocess both the query and the documents using variety of techniques. For example, we can stem words to only include base word, tokenize word using different delimiters (not only using the space as the delimiter), remove stop words, and many more.</p>

<p>For a match between a query and a document, we can determine whether we want full match of all query term, partial match only, or any other heuristics. Full match means that all query terms must be present in the document. Partial match means that at least one query term must be present in the document. These heuristics give you some control on which criteria you want the documents to be returned.</p>

<p><img src="/assets/resized/search-match-1400x689.png" alt="Figure 3: Inverted index match case." /></p>

<p><em>Figure 3: Inverted index match case.</em></p>

<hr />

<h1 id="retrieval_score">Assigning score to the retrieved documents</h1>

<p>We need to assign a score to each of the retrieved documents so that we can get the rank from the most relevant items the least relevant items with respect to the query term. There are many ways to assign score to each retrieved documents, from the older approach like tf-idf and bm25 to the more neural network approach. In this post, we’ll take a look at tf-idf.</p>

<h2 id="tf-idf-scoring">TF-IDF scoring</h2>

<p>For the following illustration, we have 4 documents in total with vocabulary size of 8: “blue, bright, can, see, shining, sky, sun, today”.</p>

<p>As the name suggest, there are two components in tf-idf: term frequency - inverse document frequency.</p>

<p>Given a word $t$ and document $d$, term frequency $f_{t,d}$ is simply the number of times each word $t$ appeared in document $d$. We can also calculate $tf(t,d)$ which is the normalized term frequency by dividing the term frequency by the total number of words in the document.</p>

<p><img src="/assets/resized/search-tf-1400x444.png" alt="Figure 4: Term frequency." /></p>

<p><em>Figure 4: Term frequency.</em></p>

<p>Inverse document frequency measures how rare $t$ is across the corpus $D$. Given $N = D$ as the total number of documents in the corpus and $n_t$ as the number of documents having $t$, the $idf(t,D)$ can be calculated as:</p>

<p><img src="/assets/resized/search-idf-1400x444.png" alt="Figure 5: Inverse document frequency." /></p>

<p><em>Figure 5: Inverse document frequency.</em></p>

<p>Given the tf and idf, we can multiply them together to get the tf-idf score.</p>

<p><img src="/assets/resized/search-tf-idf-1400x631.png" alt="Figure 6: tf-idf final score." /></p>

<p><em>Figure 6: tf-idf final score.</em></p>

<h2 id="calculate-tf-idf-score-for-new-query">Calculate tf-idf score for new query</h2>

<p>Say that your system is running in production, how does it work when you have new query coming in? For term frequency (tf) we need to calculate it real-time. For idf, we can just use the precomputed idf that we store somewhere in the system. See the illustration below:</p>

<p><img src="/assets/resized/search-rerank-1400x510.png" alt="Figure 7: Reranked list with tf-idf score." /></p>

<p><em>Figure 7: Reranked list with tf-idf score.</em></p>

<p>Once we have the tf-idf vector, we still need to calculate the similarity between the query and the matched product tf-idf vectors. The tf-idf vector of the matched product is precomputed, so we do not have to recompute this. For the similarity metric, most library uses cosine similarity. This step is $\mathcal{O}(m \cdot k)$ where $m$ is the number of matched products and $k$ is the dimension of the tf-idf vector.</p>

<p>In reality though, most of the values are zero because a single document or query only uses a tiny fraction of the total vocabulary. So although the textbook definition of a tf-idf vector is a vector of size $k$ vocabulary, in practice it is stored as a sparse vector. Instead of a fixed-size array of $k$ vocabulary size, we store a Dictionary/Map of only the non-zero indices.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Query: {term_id_42: 0.5, term_id_889: 0.8}
Document: {term_id_12: 0.1, term_id_42: 0.4}
</code></pre></div></div>

<p>So when calculating the cosine similarity, we only need to iterate through the non-zero indices of the query vector. In this case, we only need to iterate through the term_id_42 and term_id_889. We don’t need to iterate through the term_id_12 because it is not present in the query vector.</p>

<p>Example Walkthrough:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Query: {42: 0.5, 889: 0.8}
Document: {12: 0.1, 42: 0.4}

1. Look at Query Key 42: It exists in the Document. 42 has a value of 0.5 in the query and 0.4 in the document.
    0.5 * 0.4 = 0.2 
    score = 0.2

2. Look at Query Key 889: It does not exist in the Document.
    skip

Final Score: 0.2
</code></pre></div></div>

<hr />

<h1 id="appendix">Appendix</h1>

<p><a href="http://www.cbrinton.net/ECE20875-2020-Spring/W10/ngrams.pdf" target="_blank" rel="noopener noreferrer">Image source</a> for Figure 4,5 and 6.</p>]]></content><author><name>Rezka Leonandya</name></author><category term="machine learning" /><category term="system design" /><summary type="html"><![CDATA[Have you ever wondered how search engine works? How can it match a query with millions of documents super fast and return them to the users?]]></summary></entry><entry><title type="html">Target rate for class imbalance</title><link href="http://localhost:4000/target-rate/" rel="alternate" type="text/html" title="Target rate for class imbalance" /><published>2022-05-25T00:00:00+07:00</published><updated>2022-05-25T00:00:00+07:00</updated><id>http://localhost:4000/target-rate</id><content type="html" xml:base="http://localhost:4000/target-rate/"><![CDATA[<p>Class imbalance is a common problem in machine learning, where the negative class greatly outnumbers the positive class (or vice versa). I recently watch a <a href="https://www.youtube.com/watch?v=rHSpab1Wi9k" target="_blank" rel="noopener noreferrer">talk from stripe</a> where they share their techniques in addressing class imbalance in a credit card fraud detection system. I decided to create a summary here and try it out for myself on a <a href="https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets" target="_blank" rel="noopener noreferrer">credit card fraud public dataset</a>.</p>

<h1 id="target-rate">Target rate</h1>
<p>The objective is to create a model which can predict whether a transaction is fraudulent. The model is a binary classifier which produces a score in the 0-1 range, where 0 indicates no fraud and 1 indicates fraud. Based on what they’ve found, training on imbalance data and validating the metrics on imbalance data produces worse result compared to training and validating on a more balanced data. Why?</p>

<p>With a binary classifier model, we need to choose a threshold that satisfies some criteria on the validation data. This threshold is used to determine whether an instance is going to predicted as fraud or not. What they want to do in their case is that they want to maximize recall while capping FPR (false positive rate).
Say we have initially a training + validation data with less than 1% fraudulent label (the positive label). If we optimize only for recall and FPR, we can get a low FPR but extremely low precision. Why is this happening? FPR is low because the denominator of the negative label is extremely huge. The model can predict a lot of false positives and still get low FPR in this case. Then during the threshold picking phase on the validation set, we can just pick a relatively low threshold, which results in high recall but extremely low precision. Hence we need to do something on the class imbalance.</p>

<p>However, if we train and validate on a balanced dataset, we are going to be faced with class imbalance again on the production. That is why we need to find a balance that works best in production.</p>

<p>Their idea is to try to create a balanced training set that works well on the extremely imbalanced production data. Basically what we want to find is a percentage of fraud (x%) and the percentage of non-fraud training data instances (100-x%) that maximizes performances on the validation data which still contains the original proportion of the class imbalance. The percentage (x) is called the target rate. Then what we do is we can use grid/exhaustive search, trying out different values of x by keeping every fraudulent dataset and downsample the non-fraudulent dataset to create the dataset according to the target rate. After that, they evaluate the performance in validation set and also the performance in production.</p>

<h1 id="my-own-experiment">My own experiment</h1>

<p>To see how helpful is this target rate idea, I decided to try it out on a <a href="https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets" target="_blank" rel="noopener noreferrer">credit card fraud public dataset</a>. The dataset is extremely imbalanced with only 0.0001 percent of fraud dataset. I try different target rate from 0.1 to 1 and train XGBoost model with the balanced training data. Then I evaluate the XGBoost model on the imbalanced validation set and below is the result I get:</p>

<p><img src="/assets/resized/target-rate-result-800x427.png" alt="Figure 1: Precision, recall, and roc scores on different target rate." /></p>

<p><em>Figure 1: Precision, recall, and roc scores on different target rate..</em></p>

<p>We can see that the graph looks wiggly, and perhaps that the best tradeoff between the three metrics is between 0.3-0.4 target rate. Now I would say that this technique is helpful to some extent if we want to do better downsampling. However, I think that the result is prone to random samples and the random seed that we initialize. So I’m not entirely convinced that this is the best method to overcome extremely imbalanced dataset.</p>

<h1 id="closing">Closing</h1>

<p>Thanks for reading! If you think I am missing something please comment below. For those interested you can check my code <a href="https://github.com/rezkaaufar/target-rate/blob/master/target-rate-imbalanced-dataset.ipynb" target="_blank" rel="noopener noreferrer">here</a>.</p>]]></content><author><name>Rezka Leonandya</name></author><category term="machine learning" /><summary type="html"><![CDATA[Class imbalance is a common problem in machine learning, where the negative class greatly outnumbers the positive class (or vice versa). I recently watch a talk from stripe where they share their techniques in addressing class imbalance in a credit card fraud detection system. I decided to create a summary here and try it out for myself on a credit card fraud public dataset.]]></summary></entry><entry><title type="html">Uncertainty in the parameters of linear regression</title><link href="http://localhost:4000/uncertainty-in-linear-regression/" rel="alternate" type="text/html" title="Uncertainty in the parameters of linear regression" /><published>2022-03-06T00:00:00+07:00</published><updated>2022-03-06T00:00:00+07:00</updated><id>http://localhost:4000/uncertainty-in-linear-regression</id><content type="html" xml:base="http://localhost:4000/uncertainty-in-linear-regression/"><![CDATA[<p>I recently found out that linear regression assumed that the output variable comes from normal distribution, which consequently turns the coefficient to have probabilistic interpretation. I was confused at first because I was taught linear regression from a machine learning model perspective: input some features and linear regression will fit a line that minimizes a certain cost function such as root mean squared error (RMSE) and then we use that for prediction. I went blank when I realized that there are probabilistic metrics associated with each coefficient. For example, if we use python libraries such as statmodels, it is going to show these probabilistic metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nf">get_rdataset</span><span class="p">(</span><span class="sh">"</span><span class="s">Duncan</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">carData</span><span class="sh">"</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">[[</span><span class="sh">'</span><span class="s">prestige</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">education</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nf">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">income</span><span class="sh">'</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">results</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="/assets/resized/statsmodels-output-800x480.png" alt="Example output of statsmodels" /></p>

<p><em>Figure 1: Example output of statsmodels.</em></p>

<p>Where does the confidence interval parameter come from? What about the std error, t statistics, and the p-value that is associated with each coefficient? Where do they come from? In this post we are going to take a look on how does this probability metrics came about.</p>

<h2 id="random-variables-in-linear-model">Random variables in linear model</h2>

<p>Probabilistic metrics exist with the notion of random variables. In linear regression, the variable of interest $y$ that we want to predict is assumed to be generated from a normal distribution. In mathematical form, it looks like this:</p>

\[\mu = X w_{true}\]

\[y \sim \mathcal{N}(\mu, \epsilon)\]

<p>This form can be rewritten as:</p>

\[y = X w_{true} + \epsilon\]

\[\epsilon \sim \mathcal{N}(0, \mathbb{I})\]

<p>where $X$ is our observed dataset, $w_{true}$ is the true parameter that we cannot observe, and $\epsilon$ is an independent noise.</p>

<p>Why does it looks like this? Why does we have to assume that $y$ sampled from a normal distribution? The short answer is because we assume linear regression to have this property. In reality, we will never know the value of $w_{true}$, hence we want to approximate $w_{true}$ by looking at the observed data $X$. To do that, we introduce a new variable called $w_{opt}$, the parameter that we can calculate from observed data. I would assume the reader is familiar with the closed form solution to find the optimal $w_{opt}$ that approximates $w_{true}$. The equation is given as:</p>

\[w_{opt} = (X^T X)^{-1} X^T Y\]

<p>where the $Y$ is the label observations in the data.
Notice the difference here: $y$ is a random variable sampled from a normal distribution, whereas $Y$ is the label observations in the data. We assume that the observed $Y$ does not equal to $X w_{true}$, but rather to $X w_{true}$ plus some corrupted gaussian noise $\epsilon$. Hence, we further assume that $Y$ is the observations that we get from sampling the random variable of interest $y$.</p>

<p>If we look closer to the equation above, we will notice another thing: $w_{opt}$ is calculated by linearly transforming two components: $X$ and $Y$. Since we model $Y$ as a sample from the random variable $y$, we can introduce another random variable $\hat{w}$ that is calculated as:</p>

\[\hat{w} = (X^T X)^{-1} X^T y\]

<p>$\hat{w}$ represents a distribution of learnt parameter values. $w_{opt}$, the variable that we use to approximate $w_{true}$, can be seen as a sample from the distribution of learn parameter values $\hat{w}$. This is because $\hat{w}$ is defined as a linear transformation from the random variable $y$, with $(X^T X)^{-1} X^T$ being the transformer matrix. And since $Y$ is a sample from $y$, $w_{opt}$ is defined by applying the same transformation to the sample $Y$.</p>

<h2 id="probability-density-function-for-hatw">Probability density function for $\hat{w}$</h2>

<p>Now we want turn $\hat{w}$ into a probability density function. To get that, we can apply multivariate gaussian linear transformation rule to $y \sim \mathcal{N}(\mu, \epsilon)$. I will not go into the details of the derivation in this post. For those interested, please see <a href="https://towardsdatascience.com/where-do-confidence-interval-in-linear-regression-come-from-the-case-of-least-square-formulation-78f3d3ac7117" target="_blank" rel="noopener noreferrer">here</a>. Applying the transformation rule yields:</p>

\[\hat{w} \sim N(w_{true}, (X^T X)^{-1} \eta^2)\]

<p>where $\eta^2$ is the variance of each single random variable $\epsilon$. It is defined by</p>

\[\epsilon \sim \mathcal{N}(0, \mathbb{I} \eta^2)\]

<p>where $\mathbb{I}$ is the identity matrix. In this case, $\epsilon$ that we introduce in $y \sim \mathcal{N}(\mu, \epsilon)$ is a multivariate Gaussian noise where it represents noise that is independent at each data point. Hence, the dimension of $\eta^2$ is $n$, where $n$ is the number of data points.</p>

<p>This leaves us with two unknown parameters that we need to define, namely $w_{true}$ and $\eta^2$, before we can get the probabilistic metrics out of $\hat{w}$.</p>

<p>For $w_{true}$, we use $w_{opt}$ as it is only sample that we observed in the training data. For $\eta^2$, we need an observable value for the calculation. Since we already know that $\hat{w}$ is transformed by the random variable $y$, we use the variance of $y$ to represent the noise part for unknown parameters $\eta^2$. By definition, the variance of $y$ is the same as the variance of $\epsilon$ which is the same as the variance of $\eta^2$. 
The variance of $y$ can be calculated by taking the predicted value $\hat{Y} = X w_{opt}$ versus the observed value $Y$. Plugging this into a standard deviation formula, we get:</p>

\[\eta^2 = \frac{1}{n-1} (\hat{Y} - Y)^T (\hat{Y} - Y)\]

<p>Note that $\eta^2$ becomes a scalar now. That’s it! We can now proceed to look at each of the probabilistic metrics</p>

<h3 id="standard-deviation">Standard deviation</h3>

<p>We have defined this above. The standard deviation of our weight distribution is given by $(X^T X)^{-1} \eta^2$. The $(X^T X)^{-1}$ matrix is of p x p dimension and $\eta^2$ is a scalar, yielding a p x p matrix, where $p$ is the number of features that we have. The variance of each feature is at the main diagonal of this matrix.</p>

<h3 id="confidence-interval">Confidence interval</h3>

<p>Since $\hat{w}$ is a multivariate Gaussian random variable, the confidence interval for each univariate random variable in $\hat{w}$ is just some standard deviation away from its mean. We use the standard deviation parameter that we have computed above to calculate the confidence interval. For the mean, we use each elements in the $w_{opt}$ vector.</p>

<h2 id="t-statistic-and-p--t">T-statistic and $P &gt; t$</h2>

<p>These two metrics measure how likely the mean of the parameter is $0$. Having a $0$ mean indicates that the feature does not contribute to predicting the target variable $Y$. The $P &gt; t$ is the p-value telling us how far is our mean parameter from $0$, represented by t-statistics. High p-value tells us that the parameter is unlikely to be meaningful for the prediction, whereas low p-value tells us that the parameter is likely to have high contribution to the prediction.</p>

<p>We calculate the t-statistics of the feature by:</p>

\[t_j = \frac{\mu_j - 0}{\eta_j}\]

<p>where $\mu_j$ is the j-th feature mean and $\eta_j$ is the j-th standard deviation of the j-th feature. To get the p-value of the j-th feature we evaluate the t-statistics under $\mathcal{N}(0,1)$</p>

<h2 id="closing">Closing</h2>

<p>There were more stuff going on under linear regression that I hadn’t realized before. I hope this post can help you in understanding where does the probabilistic metrics came from.</p>]]></content><author><name>Rezka Leonandya</name></author><category term="machine learning" /><category term="statistics" /><summary type="html"><![CDATA[I recently found out that linear regression assumed that the output variable comes from normal distribution, which consequently turns the coefficient to have probabilistic interpretation. I was confused at first because I was taught linear regression from a machine learning model perspective: input some features and linear regression will fit a line that minimizes a certain cost function such as root mean squared error (RMSE) and then we use that for prediction. I went blank when I realized that there are probabilistic metrics associated with each coefficient. For example, if we use python libraries such as statmodels, it is going to show these probabilistic metrics.]]></summary></entry><entry><title type="html">Hypothesis testing with binomial distribution in an AB test</title><link href="http://localhost:4000/ab-test-hypothesis-testing-binomial/" rel="alternate" type="text/html" title="Hypothesis testing with binomial distribution in an AB test" /><published>2021-12-28T00:00:00+07:00</published><updated>2021-12-28T00:00:00+07:00</updated><id>http://localhost:4000/ab-test-hypothesis-testing-binomial</id><content type="html" xml:base="http://localhost:4000/ab-test-hypothesis-testing-binomial/"><![CDATA[<p>In the previous post we talked about how does frequentist hypothesis testing work in an AB test using a normal distribution. In this post we are going to look at the hypothesis testing if your variable of interest is binary using a binomial distribution. Let’s get started!</p>

<h1 id="samples--parameter-inference">Samples &amp; Parameter Inference</h1>

<p>Let’s say that we have completed an AB test and we have sample size $n=10000$ for both the control and variant group. The control have $ns_c=5500$ and the variant have $ns_v=5700$, which represents the number of success we get.</p>

<p>To do a hypothesis test, we need to infer the parameter of the binomial distribution for both the control and variant group. In binomial distribution, the parameter is $\theta$, which represents the probability of getting a positive outcome in a trial. Based on the data, we can infer that:</p>

\[p_c = \frac{ns_c}{n} = \frac{5500}{10000} = 0.55\]

\[p_v = \frac{ns_v}{n} = \frac{5700}{10000} = 0.57\]

<h1 id="binomial-distribution--significance-testing">Binomial Distribution &amp; Significance Testing</h1>

<p>Now we want to determine whether the difference between control and variant group is significant enough so that we can decide which group we want to apply in our system. To do this, we need to find out what is the probability of getting 5700 success (the variant group parameter) assuming that the control distribution with $p_c=0.55$ is true. Mathematically, it is defined as:</p>

\[a(ns) = \sum_{K=ns}^N {N \choose K} P_{0}^{K} (1 - P_{0}^{K})^{N-K}\]

<p>where in this case $ns$ is the variant group successes $ns_v$ and $P_0$ is the control group binomial distribution parameter $p_c$.</p>

<p>In plain words, the formula tells us what is the probability of observing the event plus the probability of observing other events that are equally rare and more extreme. In this case, the event is the variant group successes $ns_v$. Also note that I am using a one-sided test because in an AB test we only care about getting an improvement. We don’t care if the variant group successes is significantly worse than control group success, even though by statistical definition it still counts as significant.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="n">stats</span><span class="p">.</span><span class="nf">binom_test</span><span class="p">(</span><span class="mi">5700</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.55</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="sh">'</span><span class="s">greater</span><span class="sh">'</span><span class="p">)</span>
<span class="mf">2.9637380107656557e-05</span>
</code></pre></div></div>

<p>Using python code above, we can see that we get a low p-value, which tells us that it is unlikely to get 5700 successes assuming that the control distribution is true. Hence we can conclude that the variant group yields better conversion.</p>

<p><img src="/assets/resized/gaussian-approx-binomial-480x300.png" alt="Figure 1: Gaussian approximation of binomial distribution of both the control and variant group." /></p>

<p><em>Figure 1: Gaussian approximation of binomial distribution of both the control and variant group..</em></p>

<p>Figure 1 shows the gaussian approximation of both the control and variant binomial distributions. By theory, we know that we can <a href="https://online.stat.psu.edu/stat414/lesson/28/28.1" target="_blank" rel="noopener noreferrer">approximate binomial distribution with a normal distribution</a>. In math notation, the parameter of the normal distribution can be calculated as:</p>

\[\mu = np\]

\[\sigma = np(1-p)\]

<p>where $n$ is the number of trials and $p$ is the probability of success in a trial.</p>

<p>We can use $\sigma$ to calculate the confidence interval around the binomial parameter:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="mf">0.55</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">SE</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">95% confidence interval of our binomial distribution is between </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">SE</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s"> and </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">SE</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="mi">95</span><span class="o">%</span> <span class="n">confidence</span> <span class="n">interval</span> <span class="n">of</span> <span class="n">our</span> <span class="n">binomial</span> <span class="n">distribution</span> <span class="ow">is</span> <span class="n">between</span> <span class="mf">0.54</span> <span class="ow">and</span> <span class="mf">0.56</span>
</code></pre></div></div>

<p>The confidence interval reinforces our initial findings that getting $0.57$ is unlikely assuming that $p_c=0.55$ is true.</p>

<h1 id="closing">Closing</h1>

<p>Thanks for reading my blog post! Hit me on twitter <a href="https://twitter.com/rezkaaufar" target="_blank" rel="noopener noreferrer">@rezkaaufar</a>.</p>]]></content><author><name>Rezka Leonandya</name></author><category term="statistics" /><category term="experimentation" /><summary type="html"><![CDATA[In the previous post we talked about how does frequentist hypothesis testing work in an AB test using a normal distribution. In this post we are going to look at the hypothesis testing if your variable of interest is binary using a binomial distribution. Let’s get started!]]></summary></entry><entry><title type="html">Hypothesis testing with normal distribution in an AB test</title><link href="http://localhost:4000/ab-test-hypothesis-testing/" rel="alternate" type="text/html" title="Hypothesis testing with normal distribution in an AB test" /><published>2021-11-21T00:00:00+07:00</published><updated>2021-11-21T00:00:00+07:00</updated><id>http://localhost:4000/ab-test-hypothesis-testing</id><content type="html" xml:base="http://localhost:4000/ab-test-hypothesis-testing/"><![CDATA[<p>AB test is one of the integral parts that a data scientist need to master. One of the goals of doing AB test is to better inform the team to help them make decision. How can we do that? In a hypothetical scenario, let’s say that we have done an AB test and we have gathered two experiment data: one for variant A (baseline) and one for variant B. One of the things that we had to consider is how to analyze the data after the test had run? How do we decide which variant is better? Deciding which variants are better is crucial because ultimately this information is going to influence the team decisions.
In this post we are going to see an overview on how we can derive conclusion of an experiment based on the frequentist school of statistic. We will use normal distribution as a distribution to work with as it is one of the most common distribution that we might encounter in a real life AB test.</p>

<h1 id="before-hypothesis-testing">Before hypothesis testing</h1>
<p>Before doing hypothesis testing, the first thing that we need to understand is that we only have samples from our AB test, not the true underlying distribution. Ideally, we want to do the hypothesis testing using the true underlying distribution. In reality, we do not know anything about the true underlying distribution. We do not know their distribution, let alone the parameters of the distribution. All we can do is to make an assumption about: 1) the form of the true distribution and 2) the approximation of the true parameter of the assumed distribution. The former is mostly decided by looking at the problem and data format, whereas the latter is based on the data samples. For example. if we are working with a normal distribution, then we need to know the true mean. We can then make an assumption that the mean can be approximated with our samples. Same thing can also be said for the standard deviation.</p>

<h1 id="samples">Samples</h1>
<p>Say that we are making an intervention to our systems and our metric of interest are a continuous value, such as response time of a customer, time it takes for customer to complete an order, etc. We have gathered the experiment data and we assume that the true baseline has mean=0.1 and the true variant b has mean=-0.1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">([</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">],</span><span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">



<img class="img-fluid rounded z-depth-1" src="" srcset="/assets/img/freq-ab-sample.png 432w" data-zoomable="" />

</div>
</div>
<div class="caption">
Figure 1: Example of an AB test sample from two variants.
</div>

<h1 id="inferring-the-parameter-standard-error">Inferring the parameter: standard error</h1>
<p>First we need to infer the parameter of the distribution. In real life we do not know the true parameter, so here we just take the mean and calculate the standard deviation and assume that these values are the true parameter:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">a</span><span class="p">.</span><span class="nf">std</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span>
<span class="p">(</span><span class="mf">0.10209145722536886</span><span class="p">,</span> <span class="mf">1.004581515836704</span><span class="p">)</span>
<span class="p">(</span><span class="o">-</span><span class="mf">0.10367983213074679</span><span class="p">,</span> <span class="mf">1.008924455645851</span><span class="p">)</span>
</code></pre></div></div>

<p>With the parameter information at hand, we can plot both distribution with their confidence interval:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a_mu</span><span class="p">,</span> <span class="n">a_std</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">a</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span>
<span class="n">a_se</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">a</span><span class="p">))</span> <span class="o">//</span> <span class="n">standard</span> <span class="n">error</span>
<span class="n">a_ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">a_mu</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">a_se</span><span class="p">,</span> <span class="n">a_mu</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">a_se</span><span class="p">)</span>

<span class="n">b_mu</span><span class="p">,</span> <span class="n">b_std</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span>
<span class="n">b_se</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">b</span><span class="p">))</span> <span class="o">//</span> <span class="n">standard</span> <span class="n">error</span>
<span class="n">b_ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">b_mu</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">b_se</span><span class="p">,</span> <span class="n">b_mu</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">b_se</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">a_mu</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">a_se</span><span class="p">,</span> <span class="n">a_mu</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">a_se</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a_mu</span><span class="p">,</span> <span class="n">a_se</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="n">a_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="n">a_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">A 95% CI</span><span class="sh">"</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">b_mu</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">b_se</span><span class="p">,</span> <span class="n">b_mu</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">b_se</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">b_mu</span><span class="p">,</span> <span class="n">b_se</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="n">b_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="n">b_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">B 95% CI</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">



<img class="img-fluid rounded z-depth-1" src="" srcset="/assets/img/freq-ab-sample-ci.png 432w" data-zoomable="" />

</div>
</div>
<div class="caption">
Figure 2: The 95% confidence interval for mean of both baseline and variant b.
</div>

<p>Since we only have one mean, which is our sample mean, we use standard error to create a confidence interval around the sample mean. The confidence interval relies on a theory called central limit theorem. This theorem states that means of experiments are normally distributed. The standard error of the mean serves as our estimate of the distribution of the experiment means. So, if we multiply it by 2 and add and subtract it from the mean of one of our experiments, we will construct a 95% confidence interval for the true mean. Note that we don’t need to restrict ourselves to the 95% confidence interval.</p>

<p>What we can see from Figure 2 is that the 95% CI of the variants don’t overlap. The lower end of the CI for variant b is above the upper end of the CI for baseline. This is evidence that our result is not by chance, and that the true mean for variant b is higher than the true mean for baseline. In other words, we can conclude that there is a significant increase in our metric when switching from baseline to variant b.</p>

<h1 id="inferring-the-parameter-bootstrap">Inferring the parameter: bootstrap</h1>
<p>There is another way to estimate the interval of the true mean. Ideally when we want to estimate the interval of the true mean, we would like to be able to simulate an experiment with multiple datasets. In other words, we would like to be able to get multiple sample means from different samples to get the mean of means. Using this mean of means, we can then create a confidence interval. This technique is commonly known as bootstrap. I am not going into the bootstrap detail in this post.</p>

<h1 id="concluding-the-test-using-hypothesis-testing">Concluding the test using hypothesis testing</h1>
<p>To solidify our conclusion regarding confidence interval, we can state a hypothesis test: is the difference in means statistically different from zero (or any other value)? To achieve this, we need to test our the difference between the two distributions against a null hypothesis. The null hypothesis in this case is a zero difference in mean, represented by a zero-centered normal distribution. To calculate the difference between the two distributions, we recall that the sum or difference of 2 independent normal distributions is also a normal distribution. The resulting mean will be the sum or difference between the two distributions, while the variance will always be the sum of the variance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">diff_mu</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">a</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
<span class="n">diff_se</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="nf">var</span><span class="p">()</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="n">a</span><span class="p">.</span><span class="nf">var</span><span class="p">()</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="n">ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">diff_mu</span> <span class="o">-</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">diff_se</span><span class="p">,</span> <span class="n">diff_mu</span> <span class="o">+</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">diff_se</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">ci</span><span class="p">)</span>

<span class="p">(</span><span class="mf">0.16824798031433202</span><span class="p">,</span> <span class="mf">0.22381185401183407</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">diff_mu</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">diff_se</span><span class="p">,</span> <span class="n">diff_mu</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">diff_se</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">diff_mu</span><span class="p">,</span> <span class="n">diff_se</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="p">.</span><span class="mi">05</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">95% CI</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">



<img class="img-fluid rounded z-depth-1" src="" srcset="/assets/img/freq-ab-95-ci.png 432w" data-zoomable="" />

</div>
</div>
<div class="caption">
Figure 3: The 95% confidence interval for the difference between the two distributions.
</div>

<p>With this at hand, we can say that we are 95% confident that the true difference between the baseline and variant b group falls between -0.22 and -0.17. We can also construct a z statistic by dividing the difference in mean by the standard error of the differences. The z statistic is a measure of how extreme the observed difference is. To further test our hypothesis that the difference between the two means is statistically different, we will assume that the opposite is true, that is, the difference is zero. This is our assumed null hypothesis. Under the null hypothesis, if the difference is indeed zero, we will see the z statistic falls between 2 standard deviations of the mean 95% of the time. If the z statistic falls outside the 2 standard deviations, then we can reject the null hypothesis and conclude that there is a difference between our two distributions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="n">diff_mu</span> <span class="o">/</span> <span class="n">diff_se</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Standard Normal</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Z statistic</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">C1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="row mt-3">
<div class="col-sm mt-3 mt-md-0">



<img class="img-fluid rounded z-depth-1" src="" srcset="/assets/img/freq-ab-z-statistics.png 432w" data-zoomable="" />

</div>
</div>
<div class="caption">
Figure 4: The z-score of our differences plotted over the null hypothesis.
</div>

<p>This looks like a highly extreme value. It is below -2 which means there is less than a 5% chance that we would see such an extreme value if there were no difference in the groups. The probability of the z statistic plus the probability of observing more extreme values under the null hypothesis are mostly known as p-values. P-values measure how unlikely it is that we are seeing a measurement if the null hypothesis is true. In our case above, we can see from the graph that our p-value is extremely low ($2.5*10^{-45}$ to be exact). This again leads us to conclude that switching from baseline to variant b causes a statistically significant improvement in our metric.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">P-value:</span><span class="sh">"</span><span class="p">,</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

<span class="nf">ttest_ind</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># the two code above will yield the same value
</span></code></pre></div></div>

<h1 id="closing">Closing</h1>
<p>To the naked eye, this might shock us as the sample distribution highly overlaps in Figure 1. Personally. I think that p-value is just one component that we can calculate to help make a decision. It should not be the sole reason to decide on a problem. There may be a case where getting an improvement from -0.01 to 0.01 is not necessarily a good thing. So please be careful and know the downside of using p-value.</p>

<p>Thanks for reading my blog post. In the next post, I will talk about frequentist hypothesis testing using a bernoulli distribution. Stay tuned!</p>

<p>Contact me on twitter <a href="https://twitter.com/rezkaaufar" target="_blank" rel="noopener noreferrer">@rezkaaufar</a></p>]]></content><author><name>Rezka Leonandya</name></author><category term="statistics" /><category term="experimentation" /><summary type="html"><![CDATA[AB test is one of the integral parts that a data scientist need to master. One of the goals of doing AB test is to better inform the team to help them make decision. How can we do that? In a hypothetical scenario, let’s say that we have done an AB test and we have gathered two experiment data: one for variant A (baseline) and one for variant B. One of the things that we had to consider is how to analyze the data after the test had run? How do we decide which variant is better? Deciding which variants are better is crucial because ultimately this information is going to influence the team decisions. In this post we are going to see an overview on how we can derive conclusion of an experiment based on the frequentist school of statistic. We will use normal distribution as a distribution to work with as it is one of the most common distribution that we might encounter in a real life AB test.]]></summary></entry><entry><title type="html">Causal model with bayesian network</title><link href="http://localhost:4000/structural-causal-model/" rel="alternate" type="text/html" title="Causal model with bayesian network" /><published>2021-08-26T00:00:00+07:00</published><updated>2021-08-26T00:00:00+07:00</updated><id>http://localhost:4000/structural-causal-model</id><content type="html" xml:base="http://localhost:4000/structural-causal-model/"><![CDATA[<p>In this <a href="https://rezkaaufar.github.io/blog/2021/language-of-causal-model/">previous post</a>, I wrote about the language of causal inference via counterfactuals. I briefly mentioned that there are at least two ways to capture the causal effect: counterfactuals and structural causal model. In this post, we are going to look at structural causal model and how we can use it to simulate what would happen if we do an intervention to a particular variable of interest. Please do note that both counterfactuals and structural causal model are highly overlapped and they share the same underlying concepts.</p>

<h1 id="simpsons-paradox-in-an-acyclic-directed-graph">Simpson’s paradox in an acyclic directed graph</h1>
<p>I have mentioned about simpson’s paradox briefly in my previous blog post. Here I am going to illustrate another perspective on how simpson’s paradox can affect the result of classical machine learning. Consider a (true) causal graph below.</p>

<p><img src="/assets/img/scm-graph.png" alt="Figure 1: An example of a true causal graph between 5 variables" /></p>

<p><em>Figure 1: An example of a true causal graph between 5 variables.</em></p>

<p>Assume that we have data with these 5 variables in a spreadsheet style (think pandas dataframe) and we are tasked to find the effect on how exposure to sun ($T$) on the illness ($Y$). Without any knowledge of causal inference, we might think to just incorporate $T$ as feature and create a linear regression model on the label $Y$. Once we have the model, we use the coefficient to gauge and measure the relationship between exposure to sun and illness. If the coefficient is a positive number, then it tells us that there is a positive relationship, and vice versa if the coefficient is a negative number.</p>

<p><img src="/assets/img/scm-regression.png" alt="Figure 2: Results of two linear regressions using different features. (&lt;a href=&quot;https://www.youtube.com/watch?v=5JsFZbGqJzc&amp;t=1516s&amp;ab_channel=ODSAIGlobal&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;credits&lt;/a&gt;)" /></p>

<p><em>Figure 2: Results of two linear regressions using different features. (<a href="https://www.youtube.com/watch?v=5JsFZbGqJzc&amp;t=1516s&amp;ab_channel=ODSAIGlobal" target="_blank" rel="noopener noreferrer">credits</a>).</em></p>

<p>Looking at Figure 2 above, if we use $T$ as the single feature to predict $Y$, we got test RMSE of 10 and a coefficient of 0.99, meaning that there is a positive relationship between exposure to sun and illness. But if we were to add $C$ (type of car owned) as feature, then we would have an even better test RMSE of 7.7, hinting that type of car owned is helpful in predictive power. But if we inspect the coefficient, we see that now $T$ and $Y$ have a negative relationship, whereas $C$ and $Y$ have a positive relationship. How can this happen? How can knowing the type of car owned results in more predictive power to illness? To the naked eye, this seems pretty unintuitive. But if we know the true causal graph we will immediately realized that knowing $C$ indirectly tells us the age of the person, which gives it the predictive power to illness. This is another example of simpson’s paradox in a acyclic directed graph which we can use to distinguish flow of correlation vs causation.</p>

<hr />

<h1 id="bayesian-network">Bayesian Network</h1>

<p>We know that strong correlation between variables do not imply a causal effect. With counterfactuals, we introduce potential outcomes $(Y1, Y0)$ to be able to capture the causal effect. With structural causal model (SCM), the goal is to model the causal interdependencies between variables. One tool that we can use to achieve this is to use probabilistic graphical models (PGM). With PGM, we can model the relationships between features. PGM as a term encompasses many different approaches. In our case, one of the graph models that we can use to capture causal model is a bayesian network. Bayesian network is a directed acyclic graph that captures the interdependencies between variables. Nodes represent random variables, whereas edges represent relationships (the conditional probabilities) between variables.</p>

<p><img src="/assets/img/scm-bn-example.png" alt="Figure 3: Illustration of a bayesian network" /></p>

<p><em>Figure 3: Illustration of a bayesian network.</em></p>

<p>In a bayesian network, the value of a node is independent of the rest of the variables in the graph given its parents. The relationship between arbitrary nodes are not necessarily causal. Therefore, we need to assume that the directed edges are the actual causal effect. This is where the unconfoundedness assumption plays in SCM. We need to believe and be sure that the bayesian network that we build have no unmeasured confounders.</p>

<h2 id="how-to-build-bayesian-network">How to build bayesian network?</h2>
<p>So now we might ask: how do we build this bayesian network? If we have a lot of features, do we need to manually specify the causal relationships between the features that we have? There are some ways in which we can utilize correlation to automatically build the bayesian network, such as the <a href="https://arxiv.org/abs/1803.01422" target="_blank" rel="noopener noreferrer">NOTEARS</a> algorithm. This automatic training is also known as structure learning. However, we can’t just blindly use the result. It’s better to have an expert to review the structure and fix the relationship (add, remove, or flip edges) in the bayesian network if deemed necessary.</p>

<p><img src="/assets/img/scm-bn-build.png" alt="Figure 4: Steps to build a bayesian network" /></p>

<p><em>Figure 4: Steps to build a bayesian network.</em></p>

<p>Figure 4 above shows the steps in building a bayesian network:</p>

<ul>
<li>First, we can use some algorithm (NOTEARS) to automatically build edges between our variables.</li>
<li>As a consequence of the learning process, the edges have weights. Hence we can remove some edges that are below a certain threshold. This results in an initial causal structure build automatically from the algorithm.</li>
<li>We can then further proceed to fix the causal structure by flipping, removing, or adding edges based on our domain knowledge.</li>
</ul>

<h2 id="what-lies-underneath-bayesian-network">What lies underneath bayesian network?</h2>
<p>In the classical literature, bayesian networks are mostly either fully discrete or fully gaussian. This means that all the nodes in the graph are either discrete or continuous. The reason is because of closure properties: fully discrete allows us to model the conditional probability distribution with conditional table and they are all jointly multinomial random variable, whereas fully gaussian allows every operation (conditional, marginal, etc) to always result in another gaussian. There are other approaches where we use a non-parametric version of bayesian network which allows us to work flexibly with different types of distributions.</p>

<h2 id="parameter-estimation-in-bayesian-network">Parameter estimation in bayesian network</h2>
<p>The structure tells us the causal relationships, and for each of these relationships, there is a distribution which tells us the probability of attaining a certain state given for any variable given the states of the parent node. In a fully discrete bayesian network, the size of the conditional probability table would be the number of states of current node multiplied by the parents state. This would be total parameter that we need to estimate from data. To estimate these parameters, one can use maximum likelihood estimation (usually just involves taking counts and fractions) which produces a point estimate or use bayesian estimation and allows us to put a prior belief on the estimate.</p>

<p><img src="/assets/img/scm-bn-trained.png" alt="Figure 5: A conditional probability distributions for each node in bayesian network" /></p>

<p><em>Figure 5: A conditional probability distributions for each node in bayesian network.</em></p>

<p>Figure 5 shows the illustration after fitting the parameters of our 3 variables on the observational data. Each node is governed by a conditional probability. If a variable does not have any parents, then it’s just its own probability.</p>

<hr />

<h1 id="the-operation-in-bayesian-network">The operation in bayesian network</h1>
<p>With a bayesian network defined, one might ask: what can do with it? There are two operations that we can apply on a bayesian networks node: conditioning and intervention. Do note that conditioning is not always the same as the causal effect, that is why it is better to always do an intervention if we are able to and especially if we want to gain insight using the bayesian network.</p>

<h2 id="conditioning">Conditioning</h2>
<p>Conditioning is an observational inference: having observed the data that we have, what would the probability of a certain variable be given that we observe some parents state. Since conditioning is observational, we can do conditioning between any arbitrary nodes without having to follow the directed edge (yes including between nodes that doesn’t have any directed arrows). Conditioning is usually done in a setting where one cannot do an intervention. 
In conditioning, we can have a non-causal correlation flowing from one node to another node if we are not careful, leading to an incorrect interpretation. In other words, conditioning between arbitrary nodes without regarding the causal effect might lead to a spurious correlation. Consider this case below:</p>

<p><img src="/assets/img/scm-ass-caus.png" alt="Figure 6: Association vs causation in a graph (&lt;a href=&quot;https://www.bradyneal.com/causal-inference-course&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;credits&lt;/a&gt;)" /></p>

<p><em>Figure 6: Association vs causation in a graph (<a href="https://www.bradyneal.com/causal-inference-course" target="_blank" rel="noopener noreferrer">credits</a>).</em></p>

<p>The true causal effect from treatment $T$ to outcome $Y$ flows through $M1$ and $M2$. However, in the case above there are two other paths in which non-causal correlation can flow from $T$ to $Y$:</p>

<ul>
<li>
$W1$, $W2$, $W3$


</li>
<li>
$X1$, $X2$, $X3$


</li>
</ul>

<p>The $W$ path is known as confounder path. In bayesian network, confounder path can mess with the treatment effect because it affect how treatment $T$ is assigned, making $Y$ also affected. The only way to measure the treatment effect correctly in this case is by conditioning on all possible confounder. In the case of bayesian network, if we intervene on $T$, then the non-causal association in the W path is automatically blocked. If we cannot do an intervention and would want to derive treatment effect from observational data, then the W path needs to be conditioned on, otherwise the non-causal association will render the causal effect incorrect. In the $W$ path, conditioning on children of colliders (either $W1$ or $W3$) also blocks the non-causal association, so it doesn’t have to be on the confounder node.</p>

<p>The $X$ path is known as colliders. Contrary to the confounder, conditioning on the collider can make non-causal association flows through the path. We will not go into details but here is an excellent <a href="http://corysimon.github.io/articles/berksons-paradox-are-handsome-men-really-jerks/" target="_blank" rel="noopener noreferrer">example of colliders</a>. The idea is that if we have colliders, we should not apply conditioning on that variable as it can mess with our interpretation.</p>

<p><img src="/assets/img/scm-cond-intv.png" alt="Figure 7: Visual illustration on the difference between conditioning and intervening (&lt;a href=&quot;https://www.bradyneal.com/causal-inference-course&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;credits&lt;/a&gt;)" /></p>

<p><em>Figure 7: Visual illustration on the difference between conditioning and intervening (<a href="https://www.bradyneal.com/causal-inference-course" target="_blank" rel="noopener noreferrer">credits</a>).</em></p>

<h2 id="intervention">Intervention</h2>
<p>To gain insight from our model, we want to query our model under different observation. With intervention, we can replace the probability distributions of a certain state. If this state has a children, then we can simulate what would have happened to the children marginal probability if we do an intervention to the node. Unlike conditioning, intervention on a node will only affect its children according to the causal direction.
Figure 8 and 9 below shows how we can do an intervention to simulate the target variable.</p>

<p><img src="/assets/img/scm-bn-before-intv.png" alt="Figure 8: Marginal probability of getting high or low grades before doing an intervention" /></p>

<p><em>Figure 8: Marginal probability of getting high or low grades before doing an intervention.</em></p>

<p>Figure 8 shows the calculation of the joint probability between Grade, Study, and School Support, and the marginal probability of Grade. Remember that $P(S)$,  $P(SS)$, and $P(G \mid S, SS)$ are calculated from data. These are the parameters that we estimated after we have the causal structure. With the estimated parameter and causal structure, we calculate the marginal probability of Grade. This is the marginal probability that we learn from data before we do any intervention.</p>

<p><img src="/assets/img/scm-bn-after-intv.png" alt="Figure 9: Marginal probability of getting high or low grades after doing an intervention on the study variable" /></p>

<p><em>Figure 9: Marginal probability of getting high or low grades after doing an intervention on the study variable.</em></p>

<p>Figure 9 shows how it would look like to Grade variable if we do an intervention on Study. Here we force $P(S)$ to hold a certain value: in plain English we can interpret this as forcing everyone to Study. We can see after intervening, the marginal probability of Grade from 0.605 to 0.875 on the marginal probability $P(G=\text{High})$. This means that we can assume that if we nudge students to study more then it would yield better grades that is good for the school.</p>

<p>Please note that when doing intervention, we assume that the change in the target variables marginal probability happens on the whole population. However, when we build the causal structure and fit the parameters for every variables, we do it on the observational data. So it’s important to make sure that we have enough data first. Ensuring that we have enough data is most commonly known as fulfilling the <a href="https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf" target="_blank" rel="noopener noreferrer">positivity/overlap</a> assumption.</p>

<hr />

<h1 id="recap-and-closing">Recap and closing</h1>
<p>In this post we cover structural causal model as a way to capture causal language. We have seen:</p>

<ul>
<li>Simpson’s paradox in a graph.</li>
<li>Directed acyclic graph, especially bayesian network, as a structure to model both association and causation relationship between variables.</li>
<li>How to build bayesian network given data.</li>
<li>Condition and intervention as operations that we can apply in bayesian network.</li>
</ul>

<p>Thanks for reading this post! Contact me on twitter for feedback <a href="https://twitter.com/rezkaaufar" target="_blank" rel="noopener noreferrer">@rezkaaufar</a></p>]]></content><author><name>Rezka Leonandya</name></author><category term="statistics" /><summary type="html"><![CDATA[In this previous post, I wrote about the language of causal inference via counterfactuals. I briefly mentioned that there are at least two ways to capture the causal effect: counterfactuals and structural causal model. In this post, we are going to look at structural causal model and how we can use it to simulate what would happen if we do an intervention to a particular variable of interest. Please do note that both counterfactuals and structural causal model are highly overlapped and they share the same underlying concepts.]]></summary></entry><entry><title type="html">Understanding the language of causal model</title><link href="http://localhost:4000/language-of-causal-model/" rel="alternate" type="text/html" title="Understanding the language of causal model" /><published>2021-07-09T00:00:00+07:00</published><updated>2021-07-09T00:00:00+07:00</updated><id>http://localhost:4000/language-of-causal-model</id><content type="html" xml:base="http://localhost:4000/language-of-causal-model/"><![CDATA[<p>“Correlation does not imply causation”</p>

<p>You might hear this jargon everywhere since it is quite pervasive if you work in data science/machine learning. Also, you might have seen this image about spurious correlation:</p>

<p><img src="/assets/img/corr-col.png" alt="Figure 1: spurious correlation (&lt;a href=&quot;https://www.kaggle.com/general/187094&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;credits&lt;/a&gt;)" /></p>

<p><em>Figure 1: spurious correlation (<a href="https://www.kaggle.com/general/187094" target="_blank" rel="noopener noreferrer">credits</a>).</em></p>

<p>But what does it mean, really? What does “correlation does not imply causation” really mean? Given a data $X$ and a target variable $Y$, can we estimate direct causal effect? If so, how can we know that we have modeled causation in a correct way? Is there a formal way to capture this English statement? Yes!</p>

<p>In causal inference, there are at least two formal ways to discuss causation: one is based on counterfactuals (or potential outcomes) and the other one is based on causal directed acyclic graph (or structural causal model). The latter involves Judea Pearl’s do-calculus. For the rest of this post I am going to use the former, which is the language of counterfactuals to draw the intuition.
Okay, enough intro. The best way to start understanding why correlation $\neq$ causation is to understand the simpson’s paradox.</p>

<hr />

<h1 id="simpsons-paradox">Simpson’s paradox</h1>

<p>Imagine we are developing a treatment for both men and women. We want to know the causal effect of the treatment. Specifically, we want to draw a conclusion from the treatment with the following possibility:</p>

<ul>
<li>Treatment is good for men (S1)</li>
<li>Treatment is good for women (S2)</li>
<li>Treatment is bad overall (S3)</li>
</ul>

<p>Now, simpson’s paradox occurs when people equate probabilistic statements with the statements (S1), (S2), and (S3) above. How does the probabilistic statements look like? Consider the data below</p>

<p><img src="/assets/img/table-causal.png" alt="Table 1: illustrative data" /></p>

<p><em>Table 1: illustrative data.</em></p>

<p>From the table, we have treatment $T$ that is binary (given and not given), feature (or covariate) $X$ that is also binary (men and women) and the outcome $Y$ that is also binary (working and not working). The numbers in the column is the sum of treatment that is working ($Y=1$). Consequently, we have the probabilistic interpretation as follows:</p>

<ul>
<li>
$$P(Y=1 | T=1, X=1) = 0.15$$ 
(Group 1)


</li>
<li>
$$P(Y=1 | T=0, X=1) = 0.10$$
(Group 2)


</li>
<li>
$$P(Y=1 | T=1, X=0) = 0.30$$
(Group 3)


</li>
<li>
$$P(Y=1 | T=0, X=0) = 0.20$$
(Group 4)


</li>
<li>
$$P(Y=1 | T=1) = 0.16$$
(Group 5)


</li>
<li>
$$P(Y=1 | T=0) = 0.19$$
(Group 6)


</li>
</ul>

<p>From the probabilistic interpretation above, we can derive three key probabilistic statements:</p>

<ul>
<li>
$$P(Y=1 | T=1, X=1) - P(Y=1 | T=0, X=1) &gt; 0$$
(P1)


</li>
<li>
$$P(Y=1 | T=1, X=0) - P(Y=1 | T=0, X=0) &gt; 0$$
(P2)


</li>
<li>
$$P(Y=1 | T=1) - P(Y=1 | T=0) &lt; 0$$
(P3)


</li>
</ul>

<p>Remember: simpson’s paradox occur when people equate probabilistic statements (P1-P3) with the English statements (S1-S3). We see from the example above, all (P1), (P2), and (P3) are true. But in causal inference, it is NOT possible for (S1), (S2), and (S3) to all be true. If the treatment is good for both men and women (S1-S2), then it should not be possible that the overall treatment is bad (S3). But why does our observation says otherwise? Again, the error is in equating (P1-P3) with (S1-S3).</p>

<p>How does this happen then? Is it because of the sample size that is not comparable between groups? Well we can still have more or less the same sample size and still fall to simpson’s paradox (<a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/MIT6_S897S19_lec14note.pdf" target="_blank" rel="noopener noreferrer">example</a>). To understand why this is happening, we need to understand the effect of confounding variable.</p>

<h1 id="confounding-variable">Confounding variable</h1>
<p>Let’s take a closer look at Table 1. Do you notice a pattern in the treatment assignment? You can see that the there are more treated men than those who doesn’t receive treatment. On the other hand, there are less treated women than those who doesn’t receive treatment. We might wonder: gender probably affects the treatment assignment, therefore there are more men who received treatment than women, which led to non-comparable groups formed between men and woman. In this case, gender is a confounding variable. Confounding variable is a common cause that is both affecting the treatment and the treatment outcome. Usually confounding variable is unobserved.</p>

<p><img src="/assets/img/confound-graph.png" alt="Figure 2: directed acyclic graph illustrating confounding variable (&lt;a href=&quot;https://www.bradyneal.com/slides/2%20-%20Potential%20Outcomes.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;credits&lt;/a&gt;)" /></p>

<p><em>Figure 2: directed acyclic graph illustrating confounding variable (<a href="https://www.bradyneal.com/slides/2%20-%20Potential%20Outcomes.pdf" target="_blank" rel="noopener noreferrer">credits</a>).</em></p>

<p>There is an information flow between the treatment to the outcome via the confounding variable, resulting in (P1-P3) to all be true. In this case, $X$ is the confounding variable: the gender.</p>

<h1 id="correlation-does-not-imply-causation-in-a-formal-way">Correlation does not imply causation (in a formal way)</h1>
<p>To capture the English statements (S1-S3) above, we use counterfactuals. We start by introducing $(Y1, Y0)$ where $Y1$ is the outcome if one is treated and $Y0$ is the outcome if one is not treated. We observe:</p>

\[Y = T Y1 + (1 - T) Y0\]

<p>To put it simply, $Y1$ denotes the outcome I WOULD observe if I WERE to take the treatment, $Y0$ denotes the outcome I WOULD observe if I WERE to not take the treatment, whereas $Y$ denotes the outcome that I do observe just in the observational data. In reality, we never observe $Y1$ and $Y0$ in the observational data on any person, that is why when we derive conclusion DIRECTLY from Y, we observe the simpson’s paradox. We’ll see more details regarding counterfactuals in the next section.</p>

<p>The correct translation of (S1-S3) is:</p>

<ul>
<li>
$$P(Y1=1 | X=1) - P(Y0=1 |X=1) &gt; 0$$ 
(C1)


</li>
<li>
$$P(Y1=1 | X=0) - P(Y0=1 |X=0) &gt; 0$$ 
(C2)


</li>
<li>
$P(Y1=1) - P(Y0=1) &lt; 0$ 
(C3)


</li>
</ul>

<p>These three statements cannot be all true. If the first two statements hold, then:</p>

\[P(Y1=1) - P(Y0=1) = \sum_{x=0}^1 [P(Y1=1 | X=x) - P(Y0=1 | X=x)] P(x)\]

<p>This is why if the treatment is good for both men and women (S1-S2) , then it is not possible that the overall treatment is bad (S3).
In summary:</p>

<ul>
- (C1) = (E1) $\neq$ (P1)
- (C2) = (E2) $\neq$ (P2)
- (C3) = (E3) $\neq$ (P3)
</ul>

<p>and (E3) cannot be true if (E1) and/or (E2) hold.
In general, we have:</p>

\[P(Y=1 | T=1, X=1) - P(Y=1 | T=0, X=1) \neq P(Y1=1 | X=1) - P(Y0=1 |X=1)\]

\[P(Y=1 | T=1, X=0) - P(Y=1 | T=0, X=0) \neq P(Y1=1 | X=0) - P(Y0=1 |X=0)\]

\[P(Y=1 | T=1) - P(Y=1 | T=0) \neq P(Y1=1) - P(Y0=1)\]

<p>In other words, correlation (left hand side) does not imply causation (right hand side). The left hand side can also be called the probabilistic quantity and the right hand side can be called causal quantity.</p>

<h1 id="counterfactuals">Counterfactuals</h1>
<p>I mentioned above that in order to capture the causal statements, we can use counterfactuals (or potential outcome). But what does it mean? In our binary treatment example, counterfactuals are the outcomes that we could have observed if we can give treatment and not give treatment to a person simultaneously. Ideally, to measure the true causal effect, we would want to put everyone in the population in both the treatment group and control group(s). Consider the men group in Table 1: the true causal effect is the difference between all 1450 people for the treatment group and all 1450 people for the control group. In other words, we want both the outcome for treatment and no treatment to be available for everyone. But in this case (and almost always) the counterfactuals cannot be observed, hence we take the naive difference between n=1400 and n=50.</p>

<p>So how do we derive the correct causal effect then? How can we get $Y1$ and $Y0$ so that we can reliably say that my treatment has an effect? Do we have to fill in all the counterfactuals so that we have the same number of instances between groups? As far as I understand, there are two ways we can derive causal effect reliably:</p>

<ul>
<li>Randomized controlled trial (or AB Test).</li>
<li>Conditioning on all possible confounding variables.</li>
</ul>

<p>The former usually does not involve in filling all the counterfactuals (which we’ll see more below). The latter approach is used in a situation where it is almost impossible to do randomized controlled trials (e.g. telling non-smoker to smoke to determine the causal effect of smoking to impotence). The latter is also mostly used on observational data. Furthermore, in the latter case we can take it further by filling the counterfactuals to get the best causal effect estimate.</p>

<h1 id="randomized-controlled-trials">Randomized controlled trials</h1>
<p>RCT/AB test removes the effects of confounding to the treatment. Directed edge from $X-&gt;T$ is removed.</p>

<p><img src="/assets/img/rct-causal.png" alt="Figure 3: effects of RCT on the edge between confounding to treatment (&lt;a href=&quot;https://www.bradyneal.com/slides/2%20-%20Potential%20Outcomes.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;credits&lt;/a&gt;)" /></p>

<p><em>Figure 3: effects of RCT on the edge between confounding to treatment (<a href="https://www.bradyneal.com/slides/2%20-%20Potential%20Outcomes.pdf" target="_blank" rel="noopener noreferrer">credits</a>).</em></p>

<p>This means that the treatment assignments are now purely random. Consequently, this makes $T$ independent of $(Y1, Y0)$, or in other words, the treatment does not affect the potential outcome anymore. Note the difference: treatment $T$ still affect the actual outcome $Y$ but the treatment DOES NOT affect the potential outcome $Y1$, $Y0$. For more intuition regarding this I highly recommend checking out <a href="https://www.bradyneal.com/causal-inference-course" target="_blank" rel="noopener noreferrer">Brady Neal’s causal inference course</a>.
With the independence, we have</p>

\[P(Y=1 | T=1, X=x) = P(Y1 = 1 | X=x)\]

<p>hence we can assume that the probabilistic quantity (P1-P1) is now the same as the causal quantity (C1-C3). Therefore, we can derive the causal effect directly from the probabilistic quantity and do your statistical test to determine whether it is significant (the usual AB Test). In RCT, afaik, we can sort of trust the outcome because of the randomized treatment assignment without having to approximate the counterfactuals.</p>

<h1 id="observational-data">Observational data</h1>
<p>In the case of most observational data, where the treatment assignment is not random, we can only recover the causal effect by CONDITIONING on all possible confounding variables.</p>

<p><img src="/assets/img/conditioning-causal.png" alt="Figure 4: effects of conditioning on confounding variable, blocking the path (&lt;a href=&quot;https://www.bradyneal.com/slides/2%20-%20Potential%20Outcomes.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;credits&lt;/a&gt;)" /></p>

<p><em>Figure 4: effects of conditioning on confounding variable, blocking the path (<a href="https://www.bradyneal.com/slides/2%20-%20Potential%20Outcomes.pdf" target="_blank" rel="noopener noreferrer">credits</a>).</em></p>

<p>Essentially, we want the independence between $T$ and $(Y1, Y0)$ to happen here but we do this via conditioning on the confounding variables. We have:</p>

\[(Y0,Y1) \perp T | X\]

<p>This condition reduces the causal effect into a probabilisitic quantity. In the above example, if we assume that gender is our confounding variable, conditioning on it would yield:</p>

<p>\(P(Y1 = 1) = \sum_x P(Y = 1 | T = 1, X=x) P(X=x)\)
, where there is only one $x$, namely gender</p>

<p>Now, to further calculate the causal effect in a non-randomized observational data, it is common to approximate the unobserved counterfactuals with a prediction. In the men group above, we can train a model to predict the unobserved counterfactuals to fill the missing data points (so we have treated men n=1450, not treated men n=1450) and then predict the average treatment effect between the two groups. There are many ways to create this approximate causal model and they usually involve a lot of assumption. For more details I highly recommend to also check <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-notes/MIT6_S897S19_lec14note.pdf" target="_blank" rel="noopener noreferrer">this MIT lecture notes</a>. The most important assumption is of no unmeasured confounders, which only holds when we observe all possible variables that influence both treatment decisions and potential outcomes. In reality, one has to consult with a domain expert to make sure that all variables that influence treatments and potential outcomes are observed.</p>

<h1 id="application-in-industry">Application in industry</h1>
<p>This is something that I am curious about, but unfortunately I haven’t found the opportunity that allowed me to implement causal inference in a work setting. So I can’t speak from experience. 
Here are some examples of causal inference that I know can be beneficial in industry:</p>

<h4 id="uplift-modeling">Uplift modeling</h4>
<p>It is aimed at quantifying the treatment effect and identifying the characteristics of individuals most likely to benefit from the benefit. With this information, one thing we can do is to better choose individuals for the next cycle. The characteristics identification is essentially a prediction of counterfactuals for a customer in an experiment setting. Afaik, uplift can also be used to generate demand by giving user more targeted coupons or voucher. Uplift modeling can be used together with AB Test (offline) or multi armed bandit (online).</p>

<h4 id="causal-dags-for-forecasting">Causal DAGs for forecasting</h4>
<p>I do not know much about this since this is probably still a new field. One example that I know of is from Lyft (check this <a href="https://www.youtube.com/watch?v=5wbLy4SDuo4&amp;ab_channel=TheTWIMLAIPodcastwithSamCharrington" target="_blank" rel="noopener noreferrer">talk</a> from Sean Taylor). At Lyft, they create a causal DAGs with prior experimental evidence to model the causal effect of things. In the causal DAGs, there are:</p>

<ul>
<li>pure parent nodes that they control (price level, how much we spend on driver incentives)</li>
<li>outcome nodes that they monitor (marketplace outcome, things that happen)</li>
<li>pure parent nodes that they do not control (how many people request driver organically)</li>
</ul>

<p>They seem to have another model (like policy variables) that produces plan. This plan is then inserted into the structural causal model to sort of simulate what would happen in the outcome nodes.</p>

<p>Sean mentioned that in the business context, they need to estimate the effects of choices they make, and making those choices are causes in both the causality and causal inference senses. The estimates produced by their causal models are inputs to decision problems. Ultimately, they still need to do some decision-making (either by humans or algorithmically) that is informed by their models. I think that they cover more complex interventions involving multiple treatments and outcomes.</p>

<p>If you know any other implementation of causal inference in industry, please do let me know!</p>

<h1 id="recap-and-closing">Recap and closing</h1>
<p>What this post covered:</p>

<ul>
<li>Correlation vs causation in a formal way.</li>
- Counterfactuals as a causal language to capture causal effect correctly. By introducing potential outcomes $(Y1, Y0)$ we see where the failure is.
<li>RCT and conditioning on confounding variables as a way to reliably calculate causal effect in an experiment.</li>
</ul>

<p>What this post doesn’t cover:</p>

<ul>
<li>List of assumptions that need to hold for the counterfactuals to work.</li>
<li>Structural causal model (Pearl): another causal language to describe and capture causal effect.</li>
</ul>

<p>Thanks for reading this post! I am a noob in causal inference so please do inform me if there is an error or mistake on twitter <a href="https://twitter.com/rezkaaufar" target="_blank" rel="noopener noreferrer">@rezkaaufar</a></p>]]></content><author><name>Rezka Leonandya</name></author><category term="statistics" /><summary type="html"><![CDATA[“Correlation does not imply causation”]]></summary></entry></feed>