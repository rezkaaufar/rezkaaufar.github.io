<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Uncertainty in the parameters of linear regression</title>
  <meta name="description" content="I recently found out that linear regression assumed that the output variable comes from normal distribution, which consequently turns the coefficient to have probabilistic interpretation. I was confused at first because I was taught linear regression from a machine learning model perspective: input some features and linear regression will fit a line that minimizes a certain cost function such as root mean squared error (RMSE) and then we use that for prediction. I went blank when I realized that there are probabilistic metrics associated with each coefficient. For example, if we use python libraries such as statmodels, it is going to show these probabilistic metrics.">
  <link rel="icon" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 64 64'%3E%3Ctext x='50%25' y='50%25' text-anchor='middle' dominant-baseline='central' font-size='52'%3Eüß†%3C/text%3E%3C/svg%3E">
  <style>body {
    max-width: 80ch;
    padding: 3em 1em;
    margin: auto;
    line-height: 1.6;
    font-size: 1.08em;
    font-family: Helvetica, Arial, sans-serif;
}

a {
    color: inherit;
}

a:hover {
    text-decoration: none;
}

img {
    max-width: 100%;
    height: auto;
}

pre {
    overflow: auto;
    background: #f7f7f7;
    padding: 0.75rem;
    border-radius: 6px;
    border: 1px solid #e5e5e5;
}

code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace;
    background: #f7f7f7;
    padding: 0.1rem 0.35rem;
    border-radius: 4px;
}

blockquote {
    border-left: 3px solid #ddd;
    padding-left: 0.8rem;
    color: #444;
    margin: 1rem 0;
}

header.site-header {
    margin-bottom: 1.5rem;
}

nav ul {
    list-style: none;
    padding: 0;
    margin: 0.4rem 0 0;
    display: flex;
    flex-wrap: wrap;
    gap: 0.6rem 1rem;
}

nav a {
    text-decoration: none;
    border-bottom: 1px solid transparent;
}

nav a.active,
nav a:hover {
    border-color: #111;
}

section { margin: 1.8rem 0; }

.meta { color: #555; font-size: 0.95rem; }

.listing { width: 100%; border-collapse: collapse; }

.listing td { padding: 0.2rem 0.15rem; vertical-align: baseline; }

.listing tr + tr td { border-top: 1px solid #eee; }

.listing .date { white-space: nowrap; padding-right: 0.8rem; color: #666; font-size: 0.95rem; }

.tag {
    display: inline-block;
    padding: 0.1rem 0.55rem;
    border: 1px solid #ddd;
    border-radius: 999px;
    font-size: 0.85rem;
    margin-right: 0.35rem;
    color: #444;
}

footer { margin-top: 2.5rem; padding-top: 1.5rem; border-top: 1px solid #eee; color: #555; font-size: 0.95rem; }

.footer-links {
    display: flex;
    flex-wrap: wrap;
    gap: 0.35rem 0.75rem;
    align-items: center;
    padding: 0;
    margin: 0.5rem 0 0;
    list-style: none;
}

.footer-links a {
    text-decoration: none;
    border-bottom: 1px solid transparent;
}

.footer-links a:hover {
    border-color: #111;
}

.icon {
    font-family: "Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol",sans-serif;
    margin-right: 0.25rem;
}

.social-links {
    display: flex;
    flex-wrap: wrap;
    gap: 0.4rem 0.8rem;
    padding: 0;
    margin: 0.5rem 0 0;
    list-style: none;
}

.social-links a {
    text-decoration: none;
    border-bottom: 1px solid transparent;
}

.social-links a:hover {
    border-color: #111;
}

@media (max-width: 640px) {
    body { font-size: 1em; }
}
</style>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1K5SB4G23C"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-1K5SB4G23C');
  </script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }
    };
  </script>
  <script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
  <main>
    <a href="/" aria-label="Home">‚Üê Home</a>
<h1>Uncertainty in the parameters of linear regression</h1>
<p class="meta">
  March  6, 2022
  
</p>

<p>I recently found out that linear regression assumed that the output variable comes from normal distribution, which consequently turns the coefficient to have probabilistic interpretation. I was confused at first because I was taught linear regression from a machine learning model perspective: input some features and linear regression will fit a line that minimizes a certain cost function such as root mean squared error (RMSE) and then we use that for prediction. I went blank when I realized that there are probabilistic metrics associated with each coefficient. For example, if we use python libraries such as statmodels, it is going to show these probabilistic metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nf">get_rdataset</span><span class="p">(</span><span class="sh">"</span><span class="s">Duncan</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">carData</span><span class="sh">"</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">[[</span><span class="sh">'</span><span class="s">prestige</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">education</span><span class="sh">'</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nf">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">income</span><span class="sh">'</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">results</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="/assets/resized/statsmodels-output-800x480.png" alt="Example output of statsmodels" /></p>

<p><em>Figure 1: Example output of statsmodels.</em></p>

<p>Where does the confidence interval parameter come from? What about the std error, t statistics, and the p-value that is associated with each coefficient? Where do they come from? In this post we are going to take a look on how does this probability metrics came about.</p>

<h2 id="random-variables-in-linear-model">Random variables in linear model</h2>

<p>Probabilistic metrics exist with the notion of random variables. In linear regression, the variable of interest $y$ that we want to predict is assumed to be generated from a normal distribution. In mathematical form, it looks like this:</p>

\[\mu = X w_{true}\]

\[y \sim \mathcal{N}(\mu, \epsilon)\]

<p>This form can be rewritten as:</p>

\[y = X w_{true} + \epsilon\]

\[\epsilon \sim \mathcal{N}(0, \mathbb{I})\]

<p>where $X$ is our observed dataset, $w_{true}$ is the true parameter that we cannot observe, and $\epsilon$ is an independent noise.</p>

<p>Why does it looks like this? Why does we have to assume that $y$ sampled from a normal distribution? The short answer is because we assume linear regression to have this property. In reality, we will never know the value of $w_{true}$, hence we want to approximate $w_{true}$ by looking at the observed data $X$. To do that, we introduce a new variable called $w_{opt}$, the parameter that we can calculate from observed data. I would assume the reader is familiar with the closed form solution to find the optimal $w_{opt}$ that approximates $w_{true}$. The equation is given as:</p>

\[w_{opt} = (X^T X)^{-1} X^T Y\]

<p>where the $Y$ is the label observations in the data.
Notice the difference here: $y$ is a random variable sampled from a normal distribution, whereas $Y$ is the label observations in the data. We assume that the observed $Y$ does not equal to $X w_{true}$, but rather to $X w_{true}$ plus some corrupted gaussian noise $\epsilon$. Hence, we further assume that $Y$ is the observations that we get from sampling the random variable of interest $y$.</p>

<p>If we look closer to the equation above, we will notice another thing: $w_{opt}$ is calculated by linearly transforming two components: $X$ and $Y$. Since we model $Y$ as a sample from the random variable $y$, we can introduce another random variable $\hat{w}$ that is calculated as:</p>

\[\hat{w} = (X^T X)^{-1} X^T y\]

<p>$\hat{w}$ represents a distribution of learnt parameter values. $w_{opt}$, the variable that we use to approximate $w_{true}$, can be seen as a sample from the distribution of learn parameter values $\hat{w}$. This is because $\hat{w}$ is defined as a linear transformation from the random variable $y$, with $(X^T X)^{-1} X^T$ being the transformer matrix. And since $Y$ is a sample from $y$, $w_{opt}$ is defined by applying the same transformation to the sample $Y$.</p>

<h2 id="probability-density-function-for-hatw">Probability density function for $\hat{w}$</h2>

<p>Now we want turn $\hat{w}$ into a probability density function. To get that, we can apply multivariate gaussian linear transformation rule to $y \sim \mathcal{N}(\mu, \epsilon)$. I will not go into the details of the derivation in this post. For those interested, please see <a href="https://towardsdatascience.com/where-do-confidence-interval-in-linear-regression-come-from-the-case-of-least-square-formulation-78f3d3ac7117" target="_blank" rel="noopener noreferrer">here</a>. Applying the transformation rule yields:</p>

\[\hat{w} \sim N(w_{true}, (X^T X)^{-1} \eta^2)\]

<p>where $\eta^2$ is the variance of each single random variable $\epsilon$. It is defined by</p>

\[\epsilon \sim \mathcal{N}(0, \mathbb{I} \eta^2)\]

<p>where $\mathbb{I}$ is the identity matrix. In this case, $\epsilon$ that we introduce in $y \sim \mathcal{N}(\mu, \epsilon)$ is a multivariate Gaussian noise where it represents noise that is independent at each data point. Hence, the dimension of $\eta^2$ is $n$, where $n$ is the number of data points.</p>

<p>This leaves us with two unknown parameters that we need to define, namely $w_{true}$ and $\eta^2$, before we can get the probabilistic metrics out of $\hat{w}$.</p>

<p>For $w_{true}$, we use $w_{opt}$ as it is only sample that we observed in the training data. For $\eta^2$, we need an observable value for the calculation. Since we already know that $\hat{w}$ is transformed by the random variable $y$, we use the variance of $y$ to represent the noise part for unknown parameters $\eta^2$. By definition, the variance of $y$ is the same as the variance of $\epsilon$ which is the same as the variance of $\eta^2$. 
The variance of $y$ can be calculated by taking the predicted value $\hat{Y} = X w_{opt}$ versus the observed value $Y$. Plugging this into a standard deviation formula, we get:</p>

\[\eta^2 = \frac{1}{n-1} (\hat{Y} - Y)^T (\hat{Y} - Y)\]

<p>Note that $\eta^2$ becomes a scalar now. That‚Äôs it! We can now proceed to look at each of the probabilistic metrics</p>

<h3 id="standard-deviation">Standard deviation</h3>

<p>We have defined this above. The standard deviation of our weight distribution is given by $(X^T X)^{-1} \eta^2$. The $(X^T X)^{-1}$ matrix is of p x p dimension and $\eta^2$ is a scalar, yielding a p x p matrix, where $p$ is the number of features that we have. The variance of each feature is at the main diagonal of this matrix.</p>

<h3 id="confidence-interval">Confidence interval</h3>

<p>Since $\hat{w}$ is a multivariate Gaussian random variable, the confidence interval for each univariate random variable in $\hat{w}$ is just some standard deviation away from its mean. We use the standard deviation parameter that we have computed above to calculate the confidence interval. For the mean, we use each elements in the $w_{opt}$ vector.</p>

<h2 id="t-statistic-and-p--t">T-statistic and $P &gt; t$</h2>

<p>These two metrics measure how likely the mean of the parameter is $0$. Having a $0$ mean indicates that the feature does not contribute to predicting the target variable $Y$. The $P &gt; t$ is the p-value telling us how far is our mean parameter from $0$, represented by t-statistics. High p-value tells us that the parameter is unlikely to be meaningful for the prediction, whereas low p-value tells us that the parameter is likely to have high contribution to the prediction.</p>

<p>We calculate the t-statistics of the feature by:</p>

\[t_j = \frac{\mu_j - 0}{\eta_j}\]

<p>where $\mu_j$ is the j-th feature mean and $\eta_j$ is the j-th standard deviation of the j-th feature. To get the p-value of the j-th feature we evaluate the t-statistics under $\mathcal{N}(0,1)$</p>

<h2 id="closing">Closing</h2>

<p>There were more stuff going on under linear regression that I hadn‚Äôt realized before. I hope this post can help you in understanding where does the probabilistic metrics came from.</p>


  </main>

  <footer>
    <div class="footer-top">
      <div>¬© <span id="copyright-year">2026</span> Rezka Leonandya.</div>
      <div class="meta">Built with Jekyll + Featherweight-inspired minimal styling.</div>
    </div>
    <div class="meta"><span id="page-size"></span> ¬∑ <span id="load-time"></span></div>
  </footer>
  <script>
    (function() {
      const sizeEl = document.getElementById('page-size');
      const timeEl = document.getElementById('load-time');
      const yearEl = document.getElementById('copyright-year');
      const update = () => {
        if (sizeEl) sizeEl.textContent = `Size: ${document.documentElement.outerHTML.length} bytes`;
        if (timeEl) timeEl.textContent = `Load time: ${Math.round(performance.now())} ms`;
        if (yearEl) yearEl.textContent = new Date().getFullYear();
      };
      if (document.readyState === 'complete') {
        update();
      } else {
        window.addEventListener('load', update, { once: true });
      }
    })();
  </script>
</body>
</html>
